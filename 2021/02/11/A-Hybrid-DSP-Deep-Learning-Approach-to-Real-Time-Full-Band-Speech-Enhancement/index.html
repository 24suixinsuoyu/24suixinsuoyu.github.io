<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="manifest" href="/images/manifest.json">
  <meta name="msapplication-config" content="/images/browserconfig.xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"24suixinsuoyu.com","root":"/","scheme":"Mist","version":"7.7.2","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Jean-Marc Valin · Mozilla Corporation · Mountain View, CA, USA · jmvalin@jmvalin.ca">
<meta property="og:type" content="article">
<meta property="og:title" content="A Hybrid DSP&#x2F;Deep Learning Approach to Real-Time Full-Band Speech Enhancement">
<meta property="og:url" content="http://24suixinsuoyu.com/2021/02/11/A-Hybrid-DSP-Deep-Learning-Approach-to-Real-Time-Full-Band-Speech-Enhancement/index.html">
<meta property="og:site_name" content="隋钟元’s Blog">
<meta property="og:description" content="Jean-Marc Valin · Mozilla Corporation · Mountain View, CA, USA · jmvalin@jmvalin.ca">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjriwwm0j30vk0kcabv.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjturj6qj30qs0443yt.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjv6s7tij30z60jitar.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjwp0dbqj30qs04caac.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjx70kx8j30nu05ojrr.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjxqw3srj30ps04kt8x.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjyadecbj30y605yq3o.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjzg8ohdj30v405q3z5.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrk0624exj30uk0u0q6x.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrk5h3v3aj30ti046dg6.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkbkhpuhj30rs03yaa8.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkff9om7j30qy03y74k.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkb540z0j30x20u0kjs.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkg75u3qj32080po43s.jpg">
<meta property="article:published_time" content="2021-02-10T16:47:07.000Z">
<meta property="article:modified_time" content="2021-02-18T04:19:43.518Z">
<meta property="article:author" content="隋钟元">
<meta property="article:tag" content="RNNoise">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Speech Enhancement">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjriwwm0j30vk0kcabv.jpg">

<link rel="canonical" href="http://24suixinsuoyu.com/2021/02/11/A-Hybrid-DSP-Deep-Learning-Approach-to-Real-Time-Full-Band-Speech-Enhancement/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement | 隋钟元’s Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">隋钟元’s Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-university"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th-list"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-english">

    <a href="/english/" rel="section"><i class="fa fa-fw fa-graduation-cap"></i>英语</a>

  </li>
        <li class="menu-item menu-item-life">

    <a href="/life/" rel="section"><i class="fa fa-fw fa-balance-scale"></i>生活</a>

  </li>
        <li class="menu-item menu-item-resume">

    <a href="/resume/" rel="section"><i class="fa fa-fw fa-user-circle-o"></i>简历</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/commonweal/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/24suixinsuoyu" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://24suixinsuoyu.com/2021/02/11/A-Hybrid-DSP-Deep-Learning-Approach-to-Real-Time-Full-Band-Speech-Enhancement/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="隋钟元">
      <meta itemprop="description" content="技术创造价值">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="隋钟元’s Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-02-11 00:47:07" itemprop="dateCreated datePublished" datetime="2021-02-11T00:47:07+08:00">2021-02-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-18 12:19:43" itemprop="dateModified" datetime="2021-02-18T12:19:43+08:00">2021-02-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E3%80%8APaper%E3%80%8B/" itemprop="url" rel="index"><span itemprop="name">《Paper》</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>17k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>15 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>Jean-Marc Valin · Mozilla Corporation · Mountain View, CA, USA · <a href="mailto:jmvalin@jmvalin.ca">jmvalin@jmvalin.ca</a></p>
</blockquote>
<a id="more"></a>

<p><strong>Abstract-Despite noise suppression being a mature area in signal processing, it remains highly dependent on fine tuning of estimator algorithms and parameters. In this paper, we demonstrate a hybrid DSP/deep learning approach to noise suppression. We focus strongly on keeping the complexity as low as possible, while still achieving high-quality enhanced speech. A deep recurrent neural network with four hidden layers is used to estimate ideal critical band gains, while a more traditional pitch filter attenuates noise between pitch harmonics. The approach achieves significantly higher quality than a traditional minimum mean squared error spectral estimator, while keeping the complexity low enough for real-time operation at 48 kHz on a low-power CPU.</strong></p>
<p><strong>Index Terms-noise suppression, recurrent neural network</strong></p>
<h3 id="Ⅰ-INTRODUCTION"><a href="#Ⅰ-INTRODUCTION" class="headerlink" title="Ⅰ. INTRODUCTION"></a>Ⅰ. INTRODUCTION</h3><p>Noise suppression has been a topic of interest since at least the 70s. Despite significant improments in quality, the high level structure has remained mostly the same. Some form of spectral estimation technique relies on a noise spectral estimator, itself driven by a voice activity detector(VAD) or similar algorithm, as shown in Fig.1. Each of the 3 components requires accurate estimators and are difficult to tune. For example, the crude initial noise estimators and the spectral estimators based on spectral subtraction[1] have been replaced by more accurate noise estimators[2],[3] and spectral amplitude estimators[4]. Despite the improvements, these estimators have remained difficult to design and have required significant manual tuning effort. That is why recent advanced in deep learning techniques are appealing for noise suppression.</p>
<p>Deep learning techniques are already being used for noise suppression[5],[6],[7],[8],[9]. Many of the proposed approached target automatic speech recognition(ASR) applications, where low latency is not required. Also, in many cases, the large size of the neural network makes a real-time implementation difficult without a GPU. In the proposed approach we focus on real-time applications(e.g. video-conference) with low complexity. We also focus on full-band(48 kHz) speech. To achieve these goals we choose a hybrid approach(Sec.Ⅱ), where we rely on proven signal processing techniques and use deep learning(Sec.Ⅲ) to replace the estimators that have traditionally been hard to correctly tune. The approach contrasts with so-called <em>end-to-end</em> systems where most or all of the signal processing operations are replaced by machine learning. These end-to-end system have clearly demonstrated the capabilities of deep learning, but they often come at the cost of significantly increased complexity.</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjriwwm0j30vk0kcabv.jpg" alt="Figure 1"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">Figure 1. High-level structure of most noise suppression algorithms.</center>

<p>We show that the proposed approach has an acceptable complexity(Sec.Ⅳ) and that it provides better quality than more conventional approaches(Sec.Ⅴ). We conclude in Sec.Ⅵ with directions for further improvements to this approach.</p>
<h3 id="Ⅱ-SIGNAL-MODEL"><a href="#Ⅱ-SIGNAL-MODEL" class="headerlink" title="Ⅱ . SIGNAL MODEL"></a>Ⅱ . SIGNAL MODEL</h3><p>We propose a hybrid approach to noise suppression. The goal is to use deep learning for the aspects of noise suppression that require careful tuning while using basic signal processing building blocks for parts that do not.</p>
<p>The main processing loop is basd on 20 ms windows with 50% overlap(10ms offset). Both analysis and synthesis use the same Vorbis window[10], which satisfies the Princen-Bradley criterion[11]. The window is defined as</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjturj6qj30qs0443yt.jpg" alt="1"></p>
<p>where <em>N</em> is the window length.</p>
<p>The signal-level block diagram for the system is shown in Fig.2. The bulk of the suppression is performed on a low-resolution spectral envelope using gains computed from a recurrent neural network(RNN). Those gains are simply the square root of the ideal ratio mask(IRM). A finer suppression step attenuates the noise between pitch harmonics using a pitch comb filter.</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjv6s7tij30z60jitar.jpg" alt="Figure 2"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">Figure 2. Block diagram.</center>

<p><em>A. Band structure</em></p>
<p>In the approach taken by [5], a neural network is used to directly estimate magnitudes of frequency bins and requires a total of 6144 hidden units and close to 10 million weights to process 8 kHz speech. Scaling to 48 kHz speech using 20 ms frames would require a network with 400 outputs(0 to 20 kHz), which clearly results in a higher complexity than we can afford.</p>
<p>One way to avoid the problem is to assume that the spectral envelopes of the speech and noise and noise are sufficiently flat to use a coarser resolution than frequency bins. Also, rather than directly estimating spectral magnitudes, we instead estimate ideal critical band gains, which have the significant advantage of being bounded between 0 and 1. We choose to divide the spectrum into the same approximation of the Bark scale[12] as the Opus codec[13] uses. That is, the bands follow the Bark scale at high frequencies, but are always at least 4 bins at low frequencies. Rather than rectangular bands, we use triangular bands, with the peak response being at the boundary between bands. This results in a total of 22 bands. Our network therefore requires only 22 output values in the [0,1] range.</p>
<p>Let w<sub>b</sub>(k) be the amplitude of band b at frequency k, we have Σ<sub>b</sub>w<sub>b</sub>(k) = 1. For a transformed signal X(k), the energy in a band is given by </p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjwp0dbqj30qs04caac.jpg" alt="2"></p>
<p>The per-band gain is defined as g<sub>b</sub></p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjx70kx8j30nu05ojrr.jpg" alt="3"></p>
<p>where E<sub>s</sub>(b) is the energy of the clean(ground truth) speech and E<sub>x</sub>(b) is the energy of the input(noisy) speech. Considering an ideal band gain g^b, the following interpolated gain is applied to each frequency bin k:</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjxqw3srj30ps04kt8x.jpg" alt="4"></p>
<p><em>B. Pitch filtering</em></p>
<p>The main disadvantage of using Bark-derived bands to compute the gain is that we cannot model finer details in the spectrum. In practice, this prevents noise suppression between pitch harmonics. As an alternative, we can use a comb filter at the pitch period to cancel the inter-harmonic noise in a similar way that speech codec post-filters operate[14]. Since the periodicity of speech signal depends heavily on frequency(especially for 48 kHz sampling rate), the pitch filter operates in the frequency domain based on a per-band filtering coefficient α<sub>b</sub>. Let P(k) be the windowed DFT of the pitch-delayed signal x(n - T), the filtering is performed by computing X(k) + α<sub>b</sub>P(k) and then renormalizing the resulting signal to have thte same energy in each band as the original signal X(k).</p>
<p>The pitch correlation for band b is defined as</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjyadecbj30y605yq3o.jpg" alt="5"></p>
<p>where R[·] donotes the real part of a complex value and .* donotes the complex conjugate. Note that for a single band, (5) would be equivalent to the time-domain pitch correlation.</p>
<p>Deriving the optimal values for the filtering coefficient α<sub>b</sub> is hard and the values that minimize mean squared error are not perceptually optimal. Instead, we use a heuristic based on the following constraints and observations. Since noise causes a decrease in the pitch correlation, we do not expect p<sub>b</sub> to be greater than gb on average, so for any band that has p<sub>b</sub> &gt; g<sbu>b</sub>, we use α<sub>b</sub> = 1. When there is no noise, we do not want to distort the signal, so when g<sub>b</sub> = 1, we use α<sub>b</sub> = 0. Similarly, when  p<sub>b</sub> = 0, we have no pitch to enhance, so α<sub>b</sub> = 0. Using the following expression for the filtering coefficient respects all these constraints with smooth behavior between them:</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrjzg8ohdj30v405q3z5.jpg" alt="6"></p>
<p>Even though we use an FIR pitch filter here, it is also possible to compute P(k) based on an IIR pitch filter of the form H(z) = 1/(1  - βz<sup>-T</sup>), attenuation between harmonics at the cost of slightly increased distortion.</p>
<p><em>C. Feature extraction</em></p>
<p>In only makes sense for the input of the network to include the log spectrum of the noisy signal based on the same bands used for the output. To improve the conditioning of the training data, we apply a DCT on the log spectrum, which results in 22 Bark-frequency cepstral codfficients(BFCC). In addition to these, we also include the temporal derivative and the second temporal derivative of the first 6 BFCCs. Since we already need to compute the pitch in (5), we compute the DCT of the pitch correlation across frequency bands and include the first 6 coefficients in our set of features. At last, we include the pitch period as well as a spectral non-stationarity metric that can help in speech detection. In total we use 42 input features.</p>
<p>Unlike the features typically used in speech recognition, these features do not use cepstral mean normalization and do include the first cepstral coefficient. The choice is deliberate given that we have to track the absolute level of the noise, but it does make the features sensitive to the absolute amplitude of the signal and to the channel frequency response. This is addressed in Sec.Ⅲ-A.</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrk0624exj30uk0u0q6x.jpg" alt="Figure 3"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">Figure 3. Architecture of the neural network, showing the feed-forward, fully connected(dense) layers and the recurrent layers, along with the activation function and the number of units for each layer.</center>

<h3 id="Ⅲ-DEEP-LEARNING-ARCHITECTURE"><a href="#Ⅲ-DEEP-LEARNING-ARCHITECTURE" class="headerlink" title="Ⅲ. DEEP LEARNING ARCHITECTURE"></a>Ⅲ. DEEP LEARNING ARCHITECTURE</h3><p>The neural network closely follows the traditional structure of noise suppression algorithms, as shown in Fig.3. The design is based on the assumption that the three recurrent layers are each responsible for one of the basic components from Fig.1. Of course, in practice the neural network is free to deviate from this assumption(and likely does to some extent). It includes a total of 215 units, 4 hidden layers, with the largest layer having 96 units. We find that increasing the number of units does not significanty improve the quality of the noise suppression. However, the loss function and the way we construct the training data have a large impact on the final quality. We find that gated recurrent unit(GRU)[15] slightly outperforms LSTM on this task, while also being simpler.</p>
<p>Despite the fact that it is not strictly necessary, the network includes a VAD output. The extra complexity cost is very small(24 additional weights) and it improves training by ensuring that the corresponding GRU indeed learns to discriminate speech from noise.</p>
<p><em>A. Training data</em></p>
<p>Since the ground truth for the gains requires both the noisy speech and the clean speech, the training data has to be constructed artificailly by adding noise to clean speech data. For speech data, we use the McGill TSP speech database(French and English) and the NTT Multi-Lingual Speech Database for Telephonometry(21 languages). Various sources of noise are used, including computer fans, office, crowd, airplane, car, train, construction. The noise is mixed at different levels to produce a wide range of signal-to-noise ratios, iincluding clean speech and noise-only segments.</p>
<p>Since we do not use epstral mean normalization, we use data augmentation to make the network robust to variations in frequency responses. This is achieved by filtering the noise and speech signal independently for each training example using a second order filter of the form</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrk5h3v3aj30ti046dg6.jpg" alt="7"></p>
<p>where each of r<sub>1</sub>…r<sub>4</sub> are random values uniformly distributed in the [-3/8, 3/8] range. Robustness to the signal amplitude is achieved by varying the final level of the mixed signal.</p>
<p>We have a total of 6 hours of speech and 4 hours of noise data, which we use to produce 140 hours of noisy speech by using various combinations of gains and filters and by resampling the data to frequencies between 40 kHz and 54 kHz.</p>
<p><em>B. Optimization process</em></p>
<p>The loss function used for training determines how the network weighs excessive attenuation versus insufficient attenuation when it cannot exactly determine the correct gains. Although it is common to use the binary cross-entropy function when optimizing for values in the [0, 1] range, this does not produce good results for the gains because it does not match their perceptual effect. For a gain estimate g<sub>b</sub> and the corresponding ground truth g<sub>b</sub>, we instead train with the loss function</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkbkhpuhj30rs03yaa8.jpg" alt="8"></p>
<p>where the exponent γ is a perceptual parameter that controls how aggressively to suppress noise. Since minimizes the mean-squared error on the log-energy, which would make the suppression too aggressive given the lack of a floor on the gain. In practice, the value γ = 1 / 2 provides a good trade-off and is equivalent to minimizing the mean squared error on the energy raised to the power 1 / 4. Sometimes, there may be no noise and no speech in a particular band. This is common either when the input is silent or at high frequency when the signal is low-pass filtered. When that happens, the ground truth gain is explicitly marked as underfined and the loss function for that gain is ignored to avoid hurting the training process.</p>
<p>For the VAD output of the network, we use the standard cross-entropy loss function. Training is performed using the Keras library with the Tensorflow backend.</p>
<p><em>C. Gain smoothing</em></p>
<p>When using the gain g<sub>b</sub> to suppress noise, the output signal can sometimes sound overly dry, lacking the minimum expected level of reverberation. The problem is easily remedied by limiting the decay of gb across frames. The smoothed gains g<sub>b</sub> are obtained as</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkff9om7j30qy03y74k.jpg" alt="9"></p>
<p>where is the filtered gain of the previous frame and the decay factor λ = 0.6 is equivalent to a reverberation time of 135 ms.</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkb540z0j30x20u0kjs.jpg" alt="Figure 4"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">Figure 4. Example of noise suppression for babble noise at 15dB SNR. Spectrogram of the noisy(top), denoised(middle), and clean(bottom) speech. For the sake of clarity, only the 0-12 kHz band is shown.</center>

<h3 id="Ⅳ-COOMPLEXITY-ANALYSIS"><a href="#Ⅳ-COOMPLEXITY-ANALYSIS" class="headerlink" title="Ⅳ. COOMPLEXITY ANALYSIS"></a>Ⅳ. COOMPLEXITY ANALYSIS</h3><p>To make it easy to deploy noise suppression algorithms, it is desirable to keep both the size and the complexity low. The size of the executable is dominated by the 87,503 weights needed to represent the 215 units in the neural networks. To keep the size as small as possible, the weights can be quantized to 8 bits with no loss of performance. This makes it possible to fit all weights in the L2 cache of a CPU.</p>
<p>Since each weight is used exactly once per frame in a multiply-add operation, the neural network requires 175,000 floating-point operations(we count a multiply-add as two operations) per frame, so 17.5 Mflops for real-time use. The IFFT and the two FFTs per frame require around 7.5 Mflops and the pitch search(which operates at 12 kHz) requires around 10 Mflops. The total complexity of the algorithm is around 40 Mflops, which is comparable to that of a full-band speech coder.</p>
<p>A non-vectorized C implementation of the algorithm requires around 1.3% of a single x86 core(Haswell i7-4800MQ) to perform 48 kHz noise suppression of a single channel. The real-time complexity of the same floating-point code on a 1.2 GHz ARM Cortex-A53 core(Raspberry Pi 3) is 14%.</p>
<p>As a comparison, the 16 kHz speech enhancement approach in [9] uses 3 hidden layers, each with 2048 units. This requires 12.5 million weights and results in a complexity of 1600 Mflops. Even if quantized to 8 bits, the weights do not fit the cache of most CPUs, requiring around 800 MB/s of memory bandwidth for real-time operation.</p>
<h3 id="Ⅴ-RESULTS"><a href="#Ⅴ-RESULTS" class="headerlink" title="Ⅴ. RESULTS"></a>Ⅴ. RESULTS</h3><p>We test the quality of the noise suppression using speech and noise data not used in the training set. We compare it to the MMSE-based noise suppressor in the SpeexDSP library. Although the noise suppression operates at 48 kHz, the output has to be resampled to 16 kHz due to the limitations of wideband PESQ[16]. The objective results in Fig.5 show a significant improvement in quality from the use of deep learning, especially for non-stationary noise types. The improvement is confirmed by casual listening of the samples. Fig.4 shows the effect of the noise suppression on an example.</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gnrkg75u3qj32080po43s.jpg" alt="Figure 5"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">Figure 5. PESQ-MQS_LQO quality evaluation for babble, car and street noise.</center>

<p>An interactive demonstration of the proposed system is available at <a href="https://people.xiph.org/~jm/demo/rnnoise/" target="_blank" rel="noopener">https://people.xiph.org/~jm/demo/rnnoise/</a>, including a real-time Javasript implementation. The software implementing the proposed system can be obtained under a BSD license at <a href="https://github.com/xiph/rnnoise/" target="_blank" rel="noopener">https://github.com/xiph/rnnoise/</a> and the results were produced using commit hash 91ef401.</p>
<h3 id="Ⅵ-CONCLUSION"><a href="#Ⅵ-CONCLUSION" class="headerlink" title="Ⅵ. CONCLUSION"></a>Ⅵ. CONCLUSION</h3><p>This paper demonstrates a noise suppression approach that combines DSP-based techniques with deep learning. By using deep learning only for the aspects of noise suppression that are hard to tune, the problem is simplified to computing only 22 ideal critical band gains, which can be done efficiently using few units. The coarse resolution of the bands is then addressed by using a simple pitch filter. The resulting low complexity makes the approach suitable for use in mobile or embeded devices and the latency is low enough for use in video-conferencing systems. We also demonstrate that the quality is significantly higher than that of a pure signal processing-based approach.</p>
<p>We believe that the technique can be easily extended to residual echo suppression, for example by adding to the input features the cepstrum of the far end signal or the filtered far end signal. Similarly, it should be applicable to microphone array post-filtering by augmenting the input features  with leakage estimates like in [17].</p>
<h3 id="Ⅶ-REFERENCES"><a href="#Ⅶ-REFERENCES" class="headerlink" title="Ⅶ. REFERENCES"></a>Ⅶ. REFERENCES</h3><p>[1] S. Boll, “Suppresion of acoustic noise in speech using spectral subtraction”, <em>IEEE Transactions on acoustics, speech, and signal processing</em>, vol. 27, no. 2, pp. 113-120, 1979.</p>
<p>[2] H.-G. Hirsch and C. Ehrlicher, “Noise estimation techniques for robust speech recognition,” in <em>Proc. ICASSP</em>, 1995, vol. 1, pp. 153-156.</p>
<p>[3] T. Gerkmann and R.C. Hendriks, “Unbiased MMSE-based noise power estimation with low complexity and low tracking delay,” <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 20, no. 4, pp. 1383-1393, 2012.</p>
<p>[4] Y. Ephraim and D. Malah, “Speech enhancement using a minimum mean-square error log-spectral amplitude estimator,” <em>IEEE Transactions on Acoustics, Speech, and Signal Processing</em>, vol. 33, no. 2, pp. 443-445, 1985.</p>
<p>[5] A. Maas, Q.V. Le, T.M. O’Neil, O. Vinyals, P. Nguyen, and A.Y. Ng, “Recurrent neural networks for noise reduction in robust ASR,” in <em>Proc. INTERSPEECH</em>, 2012.</p>
<p>[6] D. Liu, P. Smaragdis, and M. Kim, “Experiments on deep learning for speech denoising,” in <em>Proc. Fifteenth Annual Conference of the International Speech Communication Association</em>, 2014.</p>
<p>[7] Y.Xu, J. Du, L.-R. Dai, and C.-H.Lee, “A regression approach to speech enhancement based on deep neural networks,” <em>IEEE Transactions on Audio, Speech and Language Processing</em>, vol. 23, no. 1, pp. 7-19, 2015.</p>
<p>[8] A. Narayanan and D. Wang, “Ideal ratio mask estimation using deep neural networks for robust speech recognition,” in <em>Proc. ICASSP</em>, 2013, pp. 7092-7096.</p>
<p>[9]  S. Mirsamadi and I. Tashev, “Causal speech enhancement combining data-driven learning and suppression rule estimation,” in <em>Proc. INTERSPEECH</em>, 2016, pp. 2087-2874.</p>
<p>[10] C. Montgomery, “Vorbis I specification,” 2004.</p>
<p>[11] J. Princen and A. Bradley, “Analysis/synthesis filter bank design based on time domain aliasing cancellation,” <em>IEEE Tran. on Acoustics, Speech, and Signal Processing</em>, vol. 34, no. 5, pp. 1153-1161, 1986.</p>
<p>[12] B.C.J. Moore, <em>An introduction to the psychology of hearing</em>, Brill, 2012.</p>
<p>[13] J.-M. Valin, G. Maxwell, T. B. Terriberry, and K. Vos, “High-quality, low-delay music coding in the Opus codec,” in <em>Proc. 135th AES Convention</em>, 2013.</p>
<p>[14] Juin-Hwey Chen and Allen Gersho, “Adaptive postfiltering for quality enhancement of coded speech,” <em>IEEE Transactions on Speech and Audio Processing</em>, vol. 3, no. 1, pp. 59-71, 1995.</p>
<p>[15] K. Cho, B. Van Merrienboer, D. Bahdanau, and Y. Bengio, “On the properties of neural machine translation: Encoder-decoder approaches,” in <em>Proc. Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation(SSST-8)</em>, 2014.</p>
<p>[16] ITU-T, <em>Perceptual evaluation of speech quality(PESQ): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs</em>, 2001.</p>
<p>[17] J.-M. Valin, J. Rouat, and F. Michaud, “Microphone array post-filter for separation of simultaneous non-stationary sources,” in <em>Proc, ICASSP</em>, 2004.</p>

    </div>

    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;margin-top:80px">-------------------- 本文结束<i class="fa fa-paw"></i>感谢您的阅读 --------------------</div>
    
</div>
      
    </div>
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>隋钟元
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://24suixinsuoyu.com/2021/02/11/A-Hybrid-DSP-Deep-Learning-Approach-to-Real-Time-Full-Band-Speech-Enhancement/" title="A Hybrid DSP&#x2F;Deep Learning Approach to Real-Time Full-Band Speech Enhancement">http://24suixinsuoyu.com/2021/02/11/A-Hybrid-DSP-Deep-Learning-Approach-to-Real-Time-Full-Band-Speech-Enhancement/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/RNNoise/" rel="tag"><i class="fa fa-tag"></i> RNNoise</a>
              <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
              <a href="/tags/Speech-Enhancement/" rel="tag"><i class="fa fa-tag"></i> Speech Enhancement</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/02/11/Callback-Hell/" rel="prev" title="Callback Hell">
      <i class="fa fa-chevron-left"></i> Callback Hell
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/02/12/Android-%E4%B8%89%E6%96%B9%E5%BA%93%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%9AGlide-%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/" rel="next" title="Android 三方库源码解析系列（一）：Glide 原理及源码解析">
      Android 三方库源码解析系列（一）：Glide 原理及源码解析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ⅰ-INTRODUCTION"><span class="nav-text">Ⅰ. INTRODUCTION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ⅱ-SIGNAL-MODEL"><span class="nav-text">Ⅱ . SIGNAL MODEL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ⅲ-DEEP-LEARNING-ARCHITECTURE"><span class="nav-text">Ⅲ. DEEP LEARNING ARCHITECTURE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ⅳ-COOMPLEXITY-ANALYSIS"><span class="nav-text">Ⅳ. COOMPLEXITY ANALYSIS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ⅴ-RESULTS"><span class="nav-text">Ⅴ. RESULTS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ⅵ-CONCLUSION"><span class="nav-text">Ⅵ. CONCLUSION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ⅶ-REFERENCES"><span class="nav-text">Ⅶ. REFERENCES</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="隋钟元"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">隋钟元</p>
  <div class="site-description" itemprop="description">技术创造价值</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">340</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">465</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:24suixinsuoyu@gmail.com" title="E-Mail → mailto:24suixinsuoyu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/24suixinsuoyu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;24suixinsuoyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/24sxsy" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;24sxsy" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/guolin_blog" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;guolin_blog" rel="noopener" target="_blank">郭霖</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/lmj623565791/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;lmj623565791&#x2F;" rel="noopener" target="_blank">鸿洋</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.520monkey.com/" title="http:&#x2F;&#x2F;www.520monkey.com&#x2F;" rel="noopener" target="_blank">姜维</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://daimajia.com/" title="https:&#x2F;&#x2F;daimajia.com&#x2F;" rel="noopener" target="_blank">代码家</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://kmxz.net/" title="https:&#x2F;&#x2F;kmxz.net&#x2F;" rel="noopener" target="_blank">孔祥舟</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://yangwenbo.com/" title="http:&#x2F;&#x2F;yangwenbo.com&#x2F;" rel="noopener" target="_blank">杨文博</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2016 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">隋钟元</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">1.4m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">21:17</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.7.2
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  















  

  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/clicklove.js"></script>

